- #### 是否合法

  - 在法律中不被禁止
  - 具有违法风险
  - 风险体现在：
    - 爬虫干扰了被访问网站的正常运营
    - 抓取了受到法律保护的特定类型的数据或信息
  - 如果避免进局子：
    - 时常优化自己的程序，避免干扰被访问网站的正常运行
    - 在使用或传播爬取到的数据时，审查爬取到的内容，发现涉及到用户隐私或者商业机密，要及时停止爬取或传播
  - 使用场景分类
    - 通用爬虫
      - 抓取系统的重要组成部分，抓取的是一整张页面数据
    - 聚焦爬虫
      - 建立在通用爬虫的基础之上，抓取的是页面中特定的局部内容
    - 增量式爬虫
      - 监测网站中数据更新的情况，只会抓取网站中最新更新出来的数据
  - 反爬机制
    - 门户网站通过制定相应的策略或者技术手段阻止爬虫程序进行网站数据的爬取
  - 反反爬策略
    - 爬虫程序也可以通过制定相关的策略或技术手段破解门户网站中的反爬机制，从而可以获取门户网站中的数据
  - robots.txt协议
    - 君子协议
    - 规定了网站中哪些数据可以被爬取，哪些不可以
    - 并没有强制规定，但很重要和权威

- #### http协议

  - 概念：服务器和客户端进行数据交互的一种形式
  - 常用请求头信息：
    - User-Agent : 请求载体的身份标识
    - Connection : 请求完毕后是断开连接还是保持连接
      - close
      - keep-alive

  - 常用响应头信息:
    - Content-Type : 服务器响应回客户端的数据类型
  - https协议：
    - 安全的http协议
    - 涉及到数据加密
  - 加密方式 : 
    - 对称秘钥加密
    - 非对称秘钥加密
    - 证书秘钥加密（https使用）

- #### requests模块

  - python中原生的一款基于网络请求的模块
  - 功能强大，使用便捷
  - 效率高
  - 作用
    - 模拟浏览器发请求
  - 如何使用（requests模块的编码流程）
    - 指定url
    - UA伪装
    - 发起请求
    - 获取响应数据
    - 持久化存储
  - 环境安装
    - pip install requests
    - 或者使用pycharm安装

- #### UA伪装

  - UA: User-Agent
  - 让爬虫对应的请求载体身份标识伪装成某一款浏览器
  - 对应的反爬策略：
    - UA检测：门户网站的服务器会检测对应请求的载体身份标识，如果为某一款浏览器，则为正常请求，如果检测到请求的载体身份标识不是某一款浏览器，则为爬虫，则很有可能拒绝该次请求



- #### json模块

  - json.dump()用来将对象转换成文本并写入文件
  - json.loads()方法可以将json字符串转换成字典

- #### 使用抓包工具分析请求很重要

- #### post请求实例demo:

  ```python
  # 导入requests模块
  import requests
  # 导入json模块
  import json
  # 判断此文件是否为主执行脚本
  if __name__ == '__main__':
      # 指定url
      post_url = 'https://fanyi.baidu.com/sug'
      
      # 输入关键字
      kw = input('enter a kw:')
      
      # post请求数据
      data = {
          'kw':kw
      }
      
      # 请求头信息设置
      headers = {
          'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.106 Safari/537.36'
      }
      
      # 发送post请求并接收结果
      response = requests.post(url=post_url,data=data,headers=headers)
      
      # 将结果转换成字典格式（只有传回的结果为json格式时可以使用，正常情况下一般使用response.text）
      result_dic_obj = response.json()
  
      # 指定文件名
      filename = kw + '.json'
      
      # 使用with语法打开文件获取fp指针
      with open(file=filename,mode='w',encoding='utf-8') as fp:
          # 将结果转换成文本格式并写入文件
          json.dump(result_dic_obj,fp,ensure_ascii=False)
  ```

  

- #### get请求实例demo:

  ```python
  # 导入requests模块
  import requests
  
  # 判断此文件是否为主执行脚本
  if __name__ == "__main__":
      # 指定url
      url = "https://www.sogou.com/web"
      
      # 输入参数
      kw = input("参数:")
      
      # 设置请求头
      headers = {
          'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.106 Safari/537.36'
      }
      
      # 设置参数
      params = {
          'query':kw
      }
      
      # 发送get请求并接收响应
      response = requests.get(url=url,params=params,headers=headers)
      
      # 获取文本格式的结果
      page_text = response.text
  
      # 设置文件名
      filename = kw + '.html'
  
      # 使用with语法打开文件获取fp指针
      with open(filename,'w',encoding="utf-8") as fp:
          # 将结果转换成文本格式并写入文件
          fp.write(page_text)
      print("爬取完成")
  ```

  

- #### Ctrl+Alt+L序列化代码

```python
# P16 0:00 ----2020年8月9日00:43:51
```

- #### 数据解析

  - 分类

    - 正则

    - bs4

    - ##### xpath

  - 原理

    - 解析的局部的文本内容都会在标签之间或者标签对应的属性中存储
    - 进行指定标签的定位
    - 标签或者标签对应属性中存储的数据值进行提取

- #### 聚焦爬虫

  - 编码流程
    - 指定url
    - 发起请求
    - 获取响应数据
    - 数据解析
    - 持久化存储

- #### 常用正则表达式

  - ![image-20200809163942401](D:\notes\随笔\note-imgs\image-20200809163942401.png)

- #### 爬取图片数据

  - response.content返回二进制形式的响应数据

- #### 判断是否发生重定向

  - 方法1:

    - ```python
      if len(response.history) > 1:
          # 发生重定向
      else:
          # 未发生重定向
      ```

      

- #### bs4

  - 使用

    - 环境安装

      - pip install bs4
      - pip install lxml

    - 实例化一个BeautifulSoup对象，并且将页面源码数据加载到该对象中（可以记载本地html文档，也可以加载互联网上的页面源码）

      - 加载本地html文档

        ```python
        import requests
        import json
        import bs4
        import lxml
        
        if __name__=='__main__':
        
            BeautifulSoup = bs4.BeautifulSoup
            
            fp = open('./sogou.html','r',encoding='utf-8')
        
            soup = BeautifulSoup(fp,'lxml')
        
            print(soup) #输出页面源码
        ```

      - 加载互联网上的页面源码

        ```python
        import requests
        import json
        import bs4
        import lxml
        
        if __name__=='__main__':
        
            BeautifulSoup = bs4.BeautifulSoup
        
            response = requests.get('https://www.baidu.com')
        
            page_text = response.text
        
            soup = BeautifulSoup(page_text,'lxml')
        
            print(soup)
        ```

    - 通过调用BeautifulSoup对象中相关的属性或者方法进行标签定位和数据提取

    - 常用属性

      - soup.tagName

        - 返回值类型:<class 'bs4.element.Tag'>

        - 返回源码中出现的第一组对应标签以及其包含的所有内容

        - demo

          ```pyt
          soup.div
          ```

      

    - 常用方法

      - 1.soup.find('tagName')

        - 返回值类型:<class 'bs4.element.Tag'>

        - 返回源码中出现的第一组对应标签以及其包含的所有内容

        - demo

          ```pyth
          soup.find('div')
          ```

      - 2.soup.find('tagName',class_/id/attr='className')

        - 返回值类型:<class 'bs4.element.Tag'>

        - 返回源码中出现的第一组对应标签以及其包含的所有内容

        - 且可以组合使用

        - demo

          ```python
          # 限制类名
          soup.find('div',class_='con')
          
          # 限制id
          soup.find('a',id='aId')
          
          # 限制属性 属性key可以是任意的
          soup.find('li',da='value')
          
          # 可随意组合使用
          soup.find('div',class_='className',id='idName'...)
          ```

      - 3.soup.find_all() 用法和find相同，只不过返回全部符合的标签，返回值为一个列表

      - 4.soup.select('selector') 返回所有符合选择器的所有标签，返回值为一个列表

    - 获取标签之间的文本数据 

      - <class 'bs4.element.Tag'>类型的对象可以使用tag.text/tag.string/tag.get_text() 属性或者方法获取
      - tag.text和tag.get_text()可以获取所有的文本内容
      - tag.string可以获取直系的文本内容

    - 获取标签的属性值

      - <class 'bs4.element.Tag'>类型的对象可以使用tag['attrName']的方式获取属性值

    - 获取标签的子标签对象

      - sub_tag = tag.tagName

- #### xpath

  - 通用解析方式，其他语言也可以使用

  - 使用：

    - 环境安装

      - pip install lxml

    - 实例化一个etree的对象，且需要将被解析的页面源码加载到该对象中

      ```python
      from lxml import etree
      ```

      - 本地源码

        ```python
        tree = etree.parse(filePath)
        ```

      - 互联网上的源码

        ```python
        tree = etree.HTML(page_text)
        ```

        

    - 调用etree对象的xpath方法结合着xpath表达式实现标签的定位和内容的捕获

    - 常用方法

      - ele = tree.xpath('xpath表达式')

- #### xpath表达式

  - 每层之间用/隔开

  - //表示多个层级

  - 开头是单/表示从根节点开始，根节点为html层的上一层

  - //如果在开头，则表示从任意位置开始定位

  - 属性定位：

    - [@attrName='value']

    - demo

      ```python
      //div[@class='value']...
      ```

  - 索引定位：

    - [index]

    - index是从1开始而非从0开始

    - demo

      ```python
      //p[2]...
      ```

  - 获取文本
    - .../text() 获取标签中直系的文本内容
    - ...//text() 获取标签下所有的文本内容

  - 获取属性值

    - .../@attrName 获取标签属性值

  - 从当前标签开始

    - demo

      ```python
      ele1 = tree.xpath('//div/div')
      
      ele2 = ele1.xpath('./h1') #从ele1标签开始执行
      ```

  - 管道符 |
    
    - 表达式1 | 表达式2 如果表达式1得到的为None,则返回表达式2对应的元素

- #### 解决乱码问题
  - response.encoding = '编码方式' 可以在提取数据前修改响应的编码方式
  - 或str = str.encode('iso-8859-1').decode('gbk')

```python
//P27 0:00 ----2020年8月9日23:26:56
```

- #### 验证码识别

  - 验证码是一种反爬机制

  - 识别验证码

    - 人工识别（不推荐使用）

    - 第三方自动识别（推荐）

      - 云打码（废了）

      - 图鉴

        - http://www.ttshitu.com/

        - demo

          ```python
          import json
          import requests
          import base64
          from io import BytesIO
          from PIL import Image
          from sys import version_info
          
          
          def base64_api(uname, pwd, img,typeid):
              img = img.convert('RGB')
              buffered = BytesIO()
              img.save(buffered, format="JPEG")
              if version_info.major >= 3:
                  b64 = str(base64.b64encode(buffered.getvalue()), encoding='utf-8')
              else:
                  b64 = str(base64.b64encode(buffered.getvalue()))
              data = {"username": uname, "password": pwd,'typeid':typeid, "image": b64}
              result = json.loads(requests.post("http://api.ttshitu.com/base64", json=data).text)
              if result['success']:
                  return result["data"]["result"]
              else:
                  return result["message"]
              return ""
          
          
          if __name__ == "__main__":
              img_path = "d:/test2.jpg"
              img = Image.open(img_path)
              result = base64_api(uname='bighand', pwd='yuanqiu0825',typeid=7, img=img)
              print(result)
          
          ```

          

- #### session

  - session = requests.Session()
  - 可以模拟session会话

- #### response.status_code可以查看响应状态码

- #### cookie与session

  - http/https无状态
  - cookie：用来让客户端保留和记录会话状态
  - 手动处理
    - 通过抓包工具获取cookie值
    - 将cookie封装到headers中（不建议使用）
  - 自动处理
    - cookie来源
      - 模拟登陆post请求后，由服务器端创建
    - session会话对象
      - 作用：可以进行请求发送
      - 如果请求过程中产生了cookie，则该cookie会被自动存储，携带在session对象中

- #### 代理

  - 破解封存IP这种反爬机制
  - 代理服务器是网络中的一个中转站
  - 代理作用：
    - 突破自身IP访问的限制
    - 隐藏自身真实IP
  - 代理相关网站
    - 快代理
    - 西祠代理
    - www.goubanjia.com

- #### 异步爬虫

  - 多线程或多进程（不是很建议使用，有更好的方法代替）

    - 好处：可以使多个阻塞操作异步进行
    - 弊端：无法无限制的开启多线程或者或进程

  - 线程池或者进程池（适当使用）

    - 好处：可以降低系统对进程或者线程创建和销毁的频率

    - 弊端：池中线程或者进程的数量是有限的

    - 使用

      - 导入线程对应的类

        ```python
        from multiprocessing.dummy import Pool
        ```

      - 实例化线程池对象

        ```python
        pool = Pool(5) #参数为初始线程数量
        ```

      - 映射

        ```python
        result_list = pool.map(func_name,iterable_obj) #iterable_obj为可迭代对象（例如列表）
        #func_name为会发生阻塞的函数
        #iterable_obj中的元素会自动分配到池中独立的线程中，作为参数传入func_name方法并执行
        #result_list为每个线程的返回结果组成的数组
        ```

        

```python
//P43 0:00 ----2020年8月11日00:37:00
```

- #### 协程

  - 叫做微线程

  - 生成器完成

  - greenlent,gevent+monkey

  - 更好地完成多任务

  - greenlet使用

    - pip install greenlet

    - ```python
      import greenlet
      def a():
          gb.swtich()
          耗时操作
      def b():
          gc.swtich()
          耗时操作
      def c():
          ga.swtich()
          耗时操作
      
      ga = greenlet(a)
      gb = greenlet(b)
      gc = greenlet(c)
      
      
      ```

  - gevent+monkey使用

    ```python
    import gevent
    from gevent import monkey
    
    monkey.patch_all() #将方法中阻塞语句偷换成可被感知的阻塞语句
    
    def a():
        耗时操作
    def b():
        耗时操作
    def c():
        耗时操作
    if __name__=="__main__":
        g1 = gevent.spawn(a)
        g2 = gevent.spawn(b)
        g3 = gevent.spawn(c)
        
        g1.join()
        g2.join()
        g3.join()
    ```

    

- #### yield用法

  - https://blog.csdn.net/mieleizhi0522/article/details/82142856/

- #### 安装依赖的方法

  - 1.命令行：pip install 依赖模块名
  - 2.使用pycharm安装依赖的步骤 ：
    - file
    - setting
    - Project: 项目名
    - Project Interpreter
    - 加号
    - 搜索
    - install

- #### python类方法的定义

  - 在方法定义上一行加上@classmethod注解即可
  - 类方法可以通过类名调用

- #### 单线程+异步协程

  - event_loop 事件循环

    - 相当于一个无限循环，可以把一些函数注册到这个时间循环上，当满足某些条件时，函数就会被循环执行

  - coroutine 协程对象

    - 我们可以将协程对象注册到事件循环中，他会被事件循环调用，可以使用async关键字来定义一个方法，这个方法在调用时不会立即被执行，而是返回一个协程对象

  - task 任务

    - 它是对协程对象的进一步封装，包含了任务的各个状态

  - future

    - 代表将来执行或还没有执行的任务，实际上和task没有本质区别
    - future.result()可以返回协程对象的返回值

  - async

    - async修饰的函数调用之后返回的是一个协程对象

  - await

    - 用来挂起阻塞语句的执行

  - demos

    - 创建协程对象，注册到事件循环，并启动

      ```python
      import asyncio
      import time
      
      #async修饰的函数调用之后返回的是一个协程对象
      async def request(url):
          print("正在请求的url:" + url)
          time.sleep(1)
          print('请求成功:' + url)
      
      #将async修饰的函数调用之后返回的是一个协程对象
      c1 = request("www.baidu.com")
      
      #创建一个事件循环对象
      loop = asyncio.get_event_loop()
      
      #将协程对象注册到loop中，然后启动loop
      loop.run_until_complete(c1)
      ```

    - task的使用

      ```python
      import asyncio
      import time
      
      #async修饰的函数调用之后返回的是一个协程对象
      async def request(url):
          print("正在请求的url:" + url)
          time.sleep(1)
          print('请求成功:' + url)
          
      #将async修饰的函数调用之后返回的是一个协程对象
      c1 = request("www.baidu.com")
      
      #task的使用
      loop = asyncio.get_event_loop()
      task1 = loop.create_task(c1)
      print(task1)
      #将task对象注册到loop中，然后启动loop
      loop.run_until_complete(task1)
      
      print(task1)
      ```

    - future的使用

      ```python
      import asyncio
      import time
      
      #async修饰的函数调用之后返回的是一个协程对象
      async def request(url):
          print("正在请求的url:" + url)
          time.sleep(1)
          print('请求成功:' + url)
      
      #将async修饰的函数调用之后返回的是一个协程对象
      c1 = request("www.baidu.com")
      
      #future的使用
      loop = asyncio.get_event_loop()
      
      future = asyncio.ensure_future(c1)
      
      print(future)
      
      loop.run_until_complete(future)
      
      print(future)
      ```

    - 绑定回调

      ```python
      import asyncio
      import time
      
      #async修饰的函数调用之后返回的是一个协程对象
      async def request(url):
          print("正在请求的url:" + url)
          time.sleep(1)
          print('请求成功:' + url)
          return url
      
      #将async修饰的函数调用之后返回的是一个协程对象
      c1 = request("www.baidu.com")
      
      #绑定回调
      def callback_func(future):
          print(future.result())
      
      loop = asyncio.get_event_loop()
      
      future = asyncio.ensure_future(c1)
      
      #将回调绑定到任务对象中
      future.add_done_callback(callback_func) #默认将任务对象传递给callback_func作为参数
      loop.run_until_complete(future)
      ```

  - 多任务协程

    ```python
    import requests
    import asyncio
    import time
    
    async def request(url):
        print("开始请求:" + url)
        #在异步协程中如果出现同步模块相关的代码，那么就无法实现异步
        # time.sleep(2)
    
        #异步模块的sleep,且当在asyncio中遇到阻塞操作，需要手动挂起(await)
        await asyncio.sleep(2)
    
        print('请求完成:' + url)
    
    urls = [
        'www.baidu.com',
        'www.google.com',
        'www.goubanjia.com'
    ]
    
    #创建任务列表
    futures = []
    for url in urls:
        c = request(url)
        future = asyncio.ensure_future(c)
        futures.append(future)
    loop = asyncio.get_event_loop()
    
    #固定写法，传入任务列表之前必须使用asyncio.wait()方法处理
    loop.run_until_complete(asyncio.wait(futures))
    ```

    

    

- #### time.time()可以获取当前时间，返回值为秒数

- #### aiohttp模块

  - 基于异步的网络请求模块

  - 解决的问题

    - requests（get，post等）是基于同步的网络请求模块，在多任务协程中会使异步无效

  - 安装:pip install aiohttp

  - 导入:import aiohttp

  - 使用该模块中的ClientSession对象进行网络请求

  - 使用:

    ```python
    import aiohttp
    
    #发送请求和读取相应都必须用await挂起操作修饰
    async with aiohttp.ClientSession() as session:
        async with await session.get(url,...) as response:#或post
            page_text = await response.text()
            bytes_result = await response.read()
            json_result = await response.json()
    ```

- #### selenium模块

  - 便捷的获取网站中动态加载的数据

  - 便捷实现模拟登陆

  - 基于浏览器自动化的一个模块

  - 使用流程

    - 环境安装: pip install selenium

    - 下载一个浏览器的驱动程序（大版本号对应即可）

    - 导包

      ```python
      from selenium import webdriver
      ```

    - 实例化一个浏览器对象

      ```python
      bro = webdriver.Chrome(executable_path='驱动程序路径') 
      #此时还会自动打开浏览器
      ```

    - 编写基于浏览器自动化的操作代码
      - bro.get(url) 让被打开的浏览器发起一个指定url的请求
      
      - page_text = bro.page_source 获取浏览器当前页面的源码数据（页面加载完后的源码数据）
      
      - bro.quit() 关闭被打开的浏览器
      
      - ele/ele_list = bro.find_XXX系列函数:可以元素定位
      
      - ele.send_keys(value) 向输入框输入值
      
      - ele.click() 点击元素
      
    - bro.execute_script(js代码) 控制浏览器执行js代码
      
      - bro.back() 后退
      
      - bro.forward() 前进
      
      - bro.save_screenshot(filepath)  保存浏览器界面截图且保存
      
      - bro.maximize_window() 使窗口最大化
      
    - 获取元素位置
      
      ```python
        img_tag = bro.find_element_by_id('J-loginImg')
      time.sleep(1) #防止未加载完成就获取
        location = img_tag.location
      ```
      
    - 获取元素大小
      
        ```python
        img_tag = bro.find_element_by_id('J-loginImg')
        time.sleep(1) #防止未加载完成就获取
        size = img_tag.size
      ```
      
      
    
  - 处理iframe
  
  - iframe内的元素无法直接用上述方法获取
    
    - bro.swtich_to.frame(iframe标签id) 切换浏览器标签定位的作用域
  - 动作链：
      - 导入：from selenium .webdriver import ActionChains
    - 实例化：action=ActionChains(bro)
      - 拖动步骤
        - 点击长按：action.click_and_hold(定位捕获的元素)
        - 鼠标移动：action.move_by_offset(x方向像素值,y方向像素).perform() ---.perform()是为了使当前动作链立即执行
    - action.release() 动作链释放
      - action.move_by_offset(x方向像素值,y方向像素).perform() 鼠标移动  
      -  ---.perform()是为了使当前动作链立即执行  
      - action.move_to_element_with_offset(ele,x,y) 移动到ele元素的相对偏移位置
      - action.click() 鼠标点击
  
  - 实现无可视化界面
  
  - 也叫无头浏览器
  
    - 导入模块：from selenium.webdriver.chrome.options import Options
  
    - 在创建浏览器对象之前添加配置
  
      ```python
      chrome_options = Options()
      chrome_options.add_argument('--headless')
      chrome_options.add_argument('--disable-gpu')
      ```
  
    - 在创建浏览器对象时添加参数chrome_options
  
      ```python
      bro = webdriver.Chrome(executable_path='驱动程序路径',chrome_options=chrome_options)
      ```
  
  - 规避使用selenium被服务器检测到的风险
  
    - 导包
  
      ```python
      from selenium.webdriver import ChromeOptions 
      ```
  
    - 实例化options对象，配置并获取浏览器对象
  
      ```python
      option = ChromeOptions()
      option.add_experimental_option('excludeSwitches',['enable-automation'])
      bro = webdriver.Chrome(executable_path='驱动程序路径',chrome_options=chrome_options,options=option)
      ```
  
      

```python
//P50 0:00 ----2020年8月11日16:46:08
```

```python
//P54 0:00 ----2020年8月12日00:36:34
```

- ##### 超级鹰

  - 官网：https://www.chaojiying.com/
  - 创建一个软件，获取软件ID

- #### 图片裁剪

  - 导包：from PIL import Image
  - 实例化对象：img = Image.open(filepath)
  - 指定范围：rangle = (startX,startY,endX,endY)
  - 裁剪：result = img.crop(rangle)
  - 保存：result.save(filepath)

- #### scrapy框架概述

  - 什么是框架
    - 就是一个集成了很多功能并且具有很强通用性的一个项目模板
  - 如何学习框架
    - 专门学习框架封装的各种功能的详细用法
  - 什么是scrapy框架
    - 爬虫中封装好的一个明星框架
  - 功能：
    - 高性能的持久化存储
    - 异步的数据下载
    - 高性能的数据解析
    - 分布式
  - 环境安装
    - pip install wheel
    - 下载twisted 
      - 下载地址：http://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted
    - 安装twisted:
      - pip install Twisted-20.3.0-cp37-cp37m-win32.whl（根据平台和python版本使用不同版本）
    - pip install pywin32
    - pip install scrapy
    - 测试：在终端里录入scrapy指令，没有报错表示安装成功

```python
//P59 0:00 ----2020年8月13日00:47:26
```

- #### scrapy基本使用

  - 创建工程
    - 命令行：scrapy startproject xxxxx(项目名称)
  - 在spiders子目录下创建一个爬虫文件：
    - 命令行：scrapy genspider spiderName www.xxx.xxx
  - settings.py配置文件
    - ROBOTSTXT_OBEY = True/False
      - 表示是否遵从robots协议
    - LOG_LEVEL 
      - 设置日志显示等级
  - 执行工程
    - 先把settings.py配置文件中的ROBOTSTXT_OBEY配置改为False
    - 命令行：scrapy crawl spiderName(爬虫文件名) 
      - 其他参数
        - --nolog 可以是控制台不打印输出日志信息

- #### scrapy数据解析

  - 在parse方法中的response常用方法
    - selector = response.xpath(xpath表达式)
      - selector对象与普通xpath结果列表元素的区别：普通xpath方法可以强转为字符串，而selector需要调用自身的extract()方法才能解析出内容
      - extract_first() 方法会让列表首元素执行extract方法，并返回结果
  - UA伪装
    - settings.py中的User-Agent配置解注释，并把浏览器的User-Agent粘贴上



- #### scrapy持久化存储

  - 基于终端指令（小众）
    - 要求：只可以将parse方法的返回值存储到本地的文本文件中，不可以存到数据库
    - 命令行：scrapy crawl 爬虫name -o [*/]xxx.csv(存储路径)
      - 存储路径后缀必须为以下选项的其中一个
      - ['json','jsonlines','jl','csv','xml','marshal','pickle']
  - 基于管道
    - 编码流程
      - 数据解析
      - 在item类（items.py脚本中）中定义相关的属性
        - 格式：field_name = scrapy.Field()
        - 在类外实例化后访问时，使用 item[field_name]格式访问
      - 将解析的数据封装存取到item类型的对象中（items.py脚本中的对象）
        - 实例化：item = XXXItem()
        - 访问实例属性：item[field_name] = xxx
        - 提交：yield item
        - 每提交一次就会调用pipelines.py中定义的process_item(self,item,spider)方法
          - 处理完之后将return item将会把item传递给下一个管道类
      - 将item类型的对象提交给管道进行持久化存储的操作
      - 在管道类的process_item()中要将其接收到的item对象中存储的数据进行持久化存储操作
      - 可以重写XXXPipeline类中的方法
        - open_spider(self,spider) 开始爬虫的时候被调用的方法
        - close_spider(self,spider) 关闭爬虫的时候被调用
      - 在配置文件中开启管道
        - settings.py文件中  
        - 给ITEM_PIPELINES添加键值对，键为完整管道类名，值为优先级，数值越小，优先级越高

```python
//64 0:00 ----2020年8月14日00:39:00
```

- #### 基于Spider的全站数据爬取

  - 就是将网站中某板块下的全部页码对应的页面数据进行爬取

  - 实现方式:

    - 将所有页面的url放到start_urls列表中（不推荐）

    - 自行手动进行请求发送

      - 在parse方法中自动请求发送

        ```python
        yield scrapy.Request(url=url,callback=self.parse)
        ```

        

- #### scrapy五大核心组件

  ![image-20200814103813422](D:\notes\随笔\note-imgs\image-20200814103813422.png)
  - Spider
    - 产生url
    - 对url进行请求发送
    - 调用parse进行数据解析
  - 调度器
    - 过滤器 （去重）
    - 队列
  - 下载器
    - 数据下载
  - 管道
    - 持久化存储 

- #### scrapy请求传参

  - 使用场景

    - 如果爬取解析的数据不在同一个页面中。（深度爬取）

  - 在scrapy.Request()方法中传递参数

    ```python
    # 在实参中添加meta字典,meta会被装进response中
    yield scrapy.Request(detail_url,callback=callback_func,meta={'key':'value',...})
    ```

  - 获取meta

    ```python
    # 在callback_func方法体中获取
    value = response.meta['key']
    ```

- #### 图片数据爬取

  - ImagesPipeline
    - 只需要将img的src的属性值进行解析，提交到管道，管道就会对图片的src进行请求发送获取图片的二进制数据并进行持久化存储
  - 使用流程

```python
//P69 0:00 ----2020年8月14日12:14:27
```

